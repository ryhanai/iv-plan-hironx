\documentclass[11pt]{jreport}
\setlength{\topmargin}{-15mm}
\setlength{\evensidemargin}{-2mm}
\setlength{\oddsidemargin}{3mm}
\setlength{\textheight}{24.5cm}
\setlength{\textwidth}{15cm}
%%
\usepackage[dvipdfm]{graphicx}
\usepackage{enumerate}

\usepackage{subfigure}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{bm} % 数式の中のBold (\bm{})
\usepackage{amsmath} % 数式の中の改行 (\begin{gather} \\ )

\usepackage{listings,jlisting}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\newcommand {\figref}[1] {図\ref{#1}}
\newcommand{\tabref}[1] {表\ref{#1}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}

\renewcommand{\subfigtopskip}{1pt}
\renewcommand{\subfigbottomskip}{1pt}
\renewcommand{\subfigcapskip}{1pt}
\setlength{\floatsep}{6pt}           % 図表と図表の間のマージン
\setlength{\dblfloatsep}{6pt}        % ↑の二段組 version
\setlength{\textfloatsep}{6pt}       % 図表と本文の間のマージン
\setlength{\abovecaptionskip}{-2pt}   % 図表の caption と図表本体の間のマージン
\setlength{\belowcaptionskip}{2pt}   % 図表の caption 下部のマージン

\input amssym.def

\author{東京大学　情報理工学系研究科　稲葉研究室}
\title{手先カメラを用いた双腕ロボットによる\\
マニピュレーションシステム\\
操作手順書}
%\date{2010/4/10}

\begin{document}
\setlength{\baselineskip}{1.5zw}

\maketitle

\tableofcontents


\chapter{システム概要}

本サービスは，工場での部品整理をイメージしたものである．
具体的には手先のカメラを用いて作業台上の部品を認識し,
両手で箱に整理して入れる機能を実現する．
手を動かすことで複数の対象物を認識，両手の干渉を考慮して
同時にアプローチできる対象物を選択する．

 \section{全体のモジュール構成}

 以下に，本システムで利用するモジュールの一覧を示す．
\begin{itemize}
 \item app-recog
 \item CameraComp
 \item LoadPictureComp
 \item iv\_plan\_hironx
 \item HiroNXInterface
\end{itemize}
ファイルシステム上の場所は必ずしも重要ではないが，
ディレクトリ構成は揃えておくと
本ドキュメントとあわせて理解しやすい．

\begin{figure}[htb]
 \begin{center}
  \includegraphics[width=0.8\linewidth]{figure/system.png}
  \caption{サービスイメージ}
  \label{fig:system}
 \end{center}
\end{figure}
\begin{figure}[htb]
 \begin{center}
  \includegraphics[width=1.0\linewidth]{figure/rtc_diagram.png}
  \caption{全体のモジュール構成}
  \label{fig:rtc_diagram}
 \end{center}
\end{figure}


\section{認識部}

\subsection{エッジベース二次元対象物認識モジュール(AppRecog)}

http://openrtm.org/openrtm/ja/project/NEDO\_Intelligent\_PRJ\_HiroAccPrj\_5002

HiroNXの手先に取り付けられたUSBカメラで対象物を認識するためのモジュール
である．カメラパラメータはデータポートを通して画像と一緒に送られてくるも
のを利用する．正しいカメラパラメータが入っていなくても対象物の認識はでき
るが，位置および姿勢は正しく推定されない．

\subsubsection{ダウンロードとコンパイル}

\begin{itemize}
 \item 現状ではgoogle code(SVN)から開発版をチェックアウト
 \item どこかのバージョンのtarを作る予定
\end{itemize}

\begin{lstlisting}
 $ cd app-recog
 $ make
\end{lstlisting}

\subsubsection{開発・動作環境}

\begin{itemize}
 \item Ubuntu Linux 10.04 LTS
 \item OpenRTM-aist 1.0.0-RELEASE C++版
 \item OpenCV 2.3
\end{itemize}

\subsubsection{インタフェース}

\begin{itemize}
 \item データポート
       \begin{itemize}
	\item 入力: Img::TimedCameraImage (Img.idl) \\
	      画像出力共通インタフェース準拠のカメラモジュールから，
	      画像及び，カメラパラメータを受取ります．
	\item 出力: TimedRecognitionResult (Vision.idl) \\
	      認識結果共通インタフェースにしたがい，対象物体の位置姿勢を出力します．
	      Img::TimedCameraImage 処理結果を画像として出力します．
       \end{itemize}
 \item サービスポート \\
       認識対象のモデルを設定するために使います．
       あらかじめ．ModelFiles/ModelList.txtにモデルIDとモデル定義ファイ
       ル名を記述し，モデルIDを引数としてサービスコールを行います．
       setModelID(i)は，i番のモデルを使用することを意味します．
\end{itemize}
 認識結果はTimedRecognitionResultによって出力されます．
 具体的な出力内容は以下の通りです．現在，対象物の姿勢以外は入っていま
 せん．
\begin{verbatim}
  0: 0, 1: 0, 2: 0, 3: 0, 4: 0
  5: 0, 6: 0, 7: 0, 
  8: R00,  9: R01, 10: R02, 11: Tx
 12: R10, 13: R11, 14: R12, 15: Ty
 16: R20, 17: R21, 18: R22, 19: Tz
\end{verbatim}

\subsubsection{カスタマイズ}

連続的に送られてくる画像に対して認識を行いますが，認識結果の時間方向の連続性
は考慮せず，各フレームで一番尤度が高い位置を計算し，その尤度が閾値以上で
あれば検出結果を返します．

% 認識手法の簡単な説明
% 閾値の意味について
% サービスによる認識対象の切り替え
% 認識モジュールにおける対象物モデルの定義の仕方

モデルと実画像のマッチングは，は画像座標系で行われます．検出した位置，姿
勢，スケールからカメラパラメータを用いて，カメラ座標系における対象物の位
置，姿勢が計算されます．
したがって，画像座標での$(x,y,\theta)$の探索範囲，検出の閾値をコンフィグ
ファイルAppRecog.confで指定します．
また，モデル定義ファイルは./ModelFileの中に置き，ファイルは頂点と辺によっ
て構成されています．

\begin{figure}[tb]
 \begin{center}
  \includegraphics[width=0.8\linewidth]{figure/apprecog.png}
  \caption{認識例}
  \label{fig:apprecog}
 \end{center}
\end{figure}

\subsection{カメラ共通I/F準拠の画像キャプチャモジュール(CameraComp)}

http://www-arailab.sys.es.osaka-u.ac.jp/CameraIF/

大阪大学により開発され画像キャプチャモジュールCameraCompをダウンロード，
コンパイルする．ログ画像によるテストを行うため，LoadPictureCompモジュー
ルも同様にダウンロードするとよい．


\subsection{認識部の動作確認}

実際にカメラモジュールと接続し，オンラインでテストを行う．
このときのモジュール接続は\figref{fig:test_by_camera}のようになり，
実行手順は，以下の通りである．
\begin{enumerate}
 \item 認識モジュールAppRecogとキャプチャモジュールをそれぞれ実行する．
 \item rtshellで画像の入出力を接続する(system editor上で操作してもよい)．
 \item 2つのRTCをactivateする．
\end{enumerate}

\begin{lstlisting}
 $ cd CameraComp
 $ ./CaptureCameraComp
 別端末で
 $ cd app-recog/
 $ build/bin/AppRecogComp
 別端末で（rtctreeでのパスは適当に補完する）
 $ rtcon CaptureCamera0.rtc:CameraImage AppRecog0.rtc:InputImage
 $ rtact CaptureCamera0.rtc AppRecog0.rtc
\end{lstlisting}

初期設定で認識範囲のスケールを絞ってあるため，認識できない場合は
対象物までの距離をいろいろ変えてみる．
また，背景に模様がなく，対象物と異なる色のものを置くと認識しやすくなる．

テストとして，カメラモジュールをLoadPictureCompモジュールに差し替え，
あらかじめ撮っておいた画像を用いてテストを行う場合，
CaptureCameraCompをLoadPictureCompに置き換えた接続となる．
ログ画像の指定は，LoadPictureCompモジュールのLoadPicture.confで
行う．AppRecogモジュール付属の画像data/parts4.jpgを用いて確認を行うとよ
い．
状態空間における探索範囲は別途指定が可能である．

% \begin{figure}[tb]
%  \begin{center}
%   %\includegraphics[width=0.5\linewidth]{figure/recog_test_log.png}
%   \caption{ログ画像を用いたテスト}
%   \label{fig:test_by_image}
%  \end{center}
% \end{figure}

\begin{figure}[tb]
 \begin{center}
  \includegraphics[width=0.8\linewidth]{figure/recog_test_cam.png}
  \caption{USBカメラを用いたテスト}
  \label{fig:test_by_camera}
 \end{center}
\end{figure}


\section{動作生成部}

\subsection{(VPython版)HiroNX動作生成モジュール}

http://openrtm.org/openrtm/ja/project/NEDO\_Intelligent\_PRJ\_HiroAccPrj\_5003

Python対話環境において、幾何モデルを用いた動作生成システムを柔軟に構築するための
スクリプト群です．RtcHandleを用いて対話環境からRTC構成によるシステムの
各モジュールと通信を行うことで，システムの統合を行います．
RRT-connectによる双腕の干渉を考慮した動作計画機能を提供し，人手に
よる動作記述とプランナによる動作生成をスムーズに統合できます．
また，RTCとして，作業共通インタフェースを実装する動作生成モジュールとし
て利用することもできます．

\subsubsection{ダウンロードとコンパイル}

install.sh
(or rosmake --rosdep-install)

\begin{lstlisting}
 $ cd iv_plan/; make
\end{lstlisting}

環境変数PYTHONPATHに iv-plan-hironx/iv\_plan/src
を追加する．

VPythonはUbuntu 10.04LTCの標準パッケージより新しいものにパッチを当て，コ
ンパイル済みのものを利用する．したがって，標準のpython-visualパッケージ
をインストールしている場合は，それがpython上で先にロードされることがない
ように注意する必要があります．

\subsubsection{開発・動作環境}

\begin{itemize}
 \item Ubuntu Linux 10.04 LTS 32bit/64bit
 \item OpenRTM-aist 1.0.0 (Python版)
\end{itemize}

\subsubsection{基本的な使い方}

まず，タスク記述，動作計画，認識および制御モジュールとの接続を
１つのPythonシェル上で行う方法を説明します．
外部RTCとの通信にはRtcHandleを利用しています．

以下のコマンドでデモプログラムを起動します．

\begin{lstlisting}[label=src:branch]
 $ cd iv_scenario/src
 $ ipython demo_wexpo.py
\end{lstlisting}


基本的に，ロボットクラス，ロボットインスタンスごとのカスタマイズは
既存クラスを拡張することで行います．
\begin{itemize}
 \item robot.pyからhironx.py
 \item hironx\_params.py
       個体差がある各種transformと，利用する外部モジュール定義
 \item 他の機能として，VRMLのローダ，センサ取り付け位置の推定機能があります
\end{itemize}

シミュレーションによる部品の箱詰め

\begin{figure}[tb]
 \begin{center}
  \includegraphics[width=0.8\linewidth]{figure/planner_scripts.png}
  \caption{動作生成モジュール}
  \label{fig:planner_scripts}
 \end{center}
\end{figure}

\subsubsection{インタフェース}


\subsection{HiroNXInterface}

http://www.openrtm.org/openrtm/en/node/4645

双腕ロボットの制御コマンドの共通インタフェースに準拠する
HiroNX用制御モジュールである．
詳細については，開発元である産業技術総合研究所のドキュメントを参照．

\subsection{動作生成部の動作確認}

HiroNXの起動は完了しているとする．

%\begin{lstlisting}[label=src:branch, caption=サンプル]
\begin{lstlisting}[label=src:branch]
 $ 起動スクリプト
 IP/ホスト名の違いはどこを直せばよいか？
 $ cd iv_scenario/src
 $ ipython demo_wexpo.py
\end{lstlisting}

\subsection{動作生成を対話環境で行う場合}

\subsection{動作生成を動作計画モジュールとして分離する場合}

シミュレーション環境
外部モジュールとの通信
動作コマンド送信，状態読み込み

\chapter{準備}

必要に応じて行うロボットごとに以下の作業を行う．

\section{モデルファイルの修正（力センサありとなし）}


\section{キャリブレーション}

ロボットの個体差を修正する作業である．
デフォルト値はHiroNX16号機のものであり，
精度を出すには各機体ごとに行う必要がある．

\subsection{カメラキャリブレーション}

RT-middlewareのコンポーネントでもROSのノードでも何でもよいので
単眼カメラのキャリブレーションを行い，CameraCompが読み込めるように
する．

その後，エッジベース二次元対象物認識モジュールにおいて，対象物の位置，距
離が正しく出力されているかどうか確認しておくとよい．

\subsection{カメラ取り付け位置のキャリブレーション}

現状では，チェッカーボードの認識にROSのノードを使用している．
キャリブレーションプログラムは，学習データとしてロボットの各姿勢における
チェッカーボードの姿勢を入力とします．チェッカーボードの姿勢はtfとして
publishされたメッセージをpythonプログラムで受信します．
しがって，準備として，
\begin{itemize}
 \item ROSのカメラノードにより画像および上記カメラパラメータがトピックと
       してpublish
 \item checkerboard detectionノードでそれらをsubscribeし，推定したtfがpublish
\end{itemize}
される状態にしておく必要があります．

\subsubsection{手順}

\begin{lstlisting}
 1, 机の上にチェッカーボードを置く

 2, 首を動かして学習データをとる
 $ ipython hironx_calib.py
 >>> res = record_data()  
 # 手先を動かして画像とそのときの関節角度値を取得する.
 # すべての姿勢でチェッカーボードが視野に入り，
 # 安定して認識できているかどうかを確認する．

 3, 頭リンクからkinectカメラへのtransformを計算する
 >>> f = calibrate(res, height=960.0)

 4, transformの書き換え
 >>> r.Thd_kinectrgb = f

 5, 確認（正しい位置でキャリブボードのフレームが止まっていれば成功）
 >>> play_data(res)

 6, デフォルト値の更新
 hironx_params.pyのThd_kinectdepthを書き換える
\end{lstlisting}

\begin{itemize}
 \item 上のコマンドは頭部kinectの場合なので，ハンドカメラの場合に変更す
       る
 \item 原理について
\end{itemize}

\chapter{デモの実行}

\section{認識部のモジュール起動と接続}

\begin{itemize}
 \item run.shを実行
       rtc.conf,左右カメラのデバイス等の設定はできているものとする．
       左右のカメラが逆になっていないかどうか確認する．
 \item 動作生成プログラムの起動
 \item look\_for
       手先を動かして机上の対象物を認識する
 \item 画像中で対象物が正しく認識されているか，認識結果がシミュレータ中
       に正しく表示されているかどうかを確認する
 \item 動作の実行
\end{itemize}


\appendix
\chapter{VPython環境でのプログラミング}

フレームやロボット，環境中の物体操作等をpython上で行う方法について説明し
ます．

\begin{verbatim}

# 0, はじめに

$ roscd MotionPlan/src
$ ipython demo.py


# 1, pythonを使う

dir(r) # 関数やメソッドの一覧
r. [TAB] # メソッド名等の補間（ipythonの機能）
help(r) # 引数や説明

# とりあえずデモ

putbox() # 箱を認識して掴み、置き直す
putbox(name='box1')
demo()

demo2() # 左手でのアプローチ

qs = gen_traj() # 手先でsinカーブを描くための関節角軌道を作る
play_traj(qs) # 作った軌道を実行する

# コマンドライン
# Ctrl+c
# Ctrl+p / Ctrl+n
# Ctrl+r


# 2, 環境中の物体操作

env
env.get_objects() # 環境中の物体一覧
[x.name for x in env.get_objects()] # 物体名一覧

# 物体は名前で管理されている。
# 同じ名前の物体は追加できないので、一度削除するか、名前を変えて追加する。
putbox() # テーブル上に箱を置く
b = env.get_object('box') # 名前で物体取得
put_box(name='box2') # 名前を変えると違う物体
env.delete_object('box2')

frm = b.where() # 物体の位置と姿勢を取得

# ロボットの移動、回転 ( world=>basejointの座標変換の変更 )
r.go_pos(-150,500,0) # 2D座標, (x,y,theta)
r.go_pos(-150,0,pi/2) # 横を向く


# 3, 自作サンプルの書き方
emacs mysample.py

== mysample.py ==
from demo import *

... 適当なコード ...
== ==

ipython mysample.py

# サンプルを修正後は、以下を実行すると
# mysample.pyの修正が反映される

import mysample # 最初の一回だけ

reload(mysample)
from mysample import *


# 4, ロボットの姿勢の操作

# joint, linkの取得
r
r.get_joints()
r.get_links()

# ジョイント名の取得
j0 = r.get_joints()[0]
dir(j0)
j0.angle
j0.name
map(lambda x: x.name, r.get_joints())
[x.name for x in r.get_joints()]

# 関節角度の表示
r.get_joint_angles()
# これは下のコードと同じ
[x.angle for x in r.get_joi

# 腕だけ
r.get_arm_joint_angles()
r.get_arm_joint_angles(arm='left')
r.set_arm_joint_angles(angles)
r.set_arm_joint_angles(angles, arm='left')

# ハンドだけ
r.get_hand_joint_angles()
r.get_hand_joint_angles(hand='left')
r.set_hand_joint_angles(angles)
r.set_hand_joint_angles(angles, hand='left')

# 関節１つを変える
r.get_joint_angle(0) # ID
r.set_joint_angle(0, 0.5) # IDと角度[rad]

# 初期姿勢に戻す
r.reset_pose()

# あらかじめいくつかの姿勢が定義されている
r.poses
r.poses.keys()

# r.reset_pose()は以下と同じ
r.set_joint_angles(r.poses['init'])

# 手のポーズ
r.hand_poses
r.hand_poses['open']
r.set_hand_joint_angles(r.hand_poses['open']) # 手を開く
r.set_hand_joint_angles(r.hand_poses['close']) # 手を閉じる

# アニメーション（首を振ってみる）
arange(0, 1, 0.2)
for th in arange(0,1,0.2):
        r.set_joint_angle(1, th)
        time.sleep(0.5)

# ちなみに左右の腕の関節書く取得は以下のコードと同じ
r.get_joint_angles()[3:9]
r.get_joint_angles()[15:19]

## 練習：
##   いろんな姿勢を作ってみよう
##   いろんな動作列を作って、アニメーションをしてみよう


# 5, 座標系(frame)について
# 3x3回転行列と3次元ベクトル = 4x4の同次数行列
# euler角, 自由軸回転, quaternion

help(VECTOR)
help(MATRIX)
help(FRAME)

# 値の生成(constructor)
VECTOR()
MATRIX()
FRAME()
v=VECTOR(vec=[100,0,0])
MATRIX(angle=pi/2, axis=VECTOR(vec=[0,0,1]))
MATRIX(c=pi/2) # a,b,cを同時に指定できないので注意
m=MATRIX([[1,0,0],[0,1,0],[0,0,1]])
FRAME()

frm.mat # 姿勢部分
frm.vec # 位置部分

# 演算
v*v # ベクトル積
dot(v,v) # 内積
v+v # 和
2*v # スカラー倍
# 行列は姿勢表現専用（直行行列）
m*m # 積
-m # 逆行列
# -mはじ実装は転置行列
# スカラー倍、行列和は定義されない(配列の結合解釈される)

# 姿勢表現間の変換
m.abc() # 行列=>euler
m.rot_axis() # 行列=>自由軸回転
# euler=>回転行列、自由軸回転=>回転行列は上記のMATRIXのコンストラクタ

# 位置と姿勢を合わせた表現（同時行列）
FRAME(mat=m, vec=v)
FRAME(xyzabc=[x,y,z,a,b,c])

f=FRAME(vec=[500,0,1000])
show_frame(f)
f.mat = MATRIX(a=pi/4)
show_frame(f)

f.mat = MATRIX(a=pi/4)*MATRIX(b=pi/4)
show_frame(f)

# 座標系の親子構造
FRAME.affix() # 座標系の親子関係の定義
FRAME.unfix() # 座標系の親子関係の削除
FRAME.set_trans() # 親子間の座標変換の設定
f.rel_trans # 親子間の座標変換の取得

# 物体追加
# putbox()の記述を参照
# 表示用形状とFRAMEを作り、適当な親座標系の子FRAMEとして、座標系ツリーに挿入する

env.insert_object('box2') # 物体追加
env.delete_object('box2') # 物体削除(子フレームの物体も削除される)

## 練習：
##  物体の位置を変えてみよう
##  テーブルを20[cm]前に出してみよう
##  ロボットを移動させてみよう
##  適当な物体を作成したり、削除してみよう


# 6, 逆運動学(Inverse Kinematics, IK)の利用

# 手首位置，姿勢の取得
r.fk() # 順運動学計算，現在の手先FRAMEを返す．デフォルトは右手．
r.fk(arm='left') # 左手は明示的に引数で指定する

# 目的位置へのアプローチ

putbox() # 手が届きそうなところに置く
objfrm = detect()

# アプローチ姿勢，把持姿勢の計算（2つずつ求まる）
afrms, gfrms = pl.grasp_plan(objfrm)

# アプローチ姿勢を表示
# (FRAMEからCoordinateObjectを作って、環境にinsert_objectする)
show_frame(afrms[0])
show_frame(afrms[1])

# 目標位置へ手を伸ばすための関節角度を計算する
r.ik(afrms)
help(r.ik)
# - IKは目標手先FRAMEの集合を引数にとり、
#   初期姿勢から関節空間での重み付き距離順に並べた解の列を返す
#   目標手先フレーム１つを引数に指定してもよい
# - 解が存在しなければNone
# - ほとんどの場合、最初の解を採用すればよい

asols = r.ik(afrms)
r.set_arm_joint_angles(asols[0]) # 腕の角度のみ

gsols = r.ik(gfrms)
r.set_arm_joint_angles(gsols[0]) # 腕の角度のみ

# 腰を使う
# 腰yaw軸を使うと手の届く範囲が大きく広がる

asols = r.ik(afrms, use_waist=True) # 返値の形式が違うので注意 (waist_yaw, arm_angles)
th,js = asols[0]
r.set_joint_angle(0,th) # 腰を回す
r.set_arm_joint_angles(js) # 腕の姿勢を変える

# 他の解も表示してみる
for th,js in asols:
        r.set_joint_angle(0,th)
        r.set_arm_joint_angles(js)
        time.sleep(0.5)

# 物体をハンドに固定する
tgtobj = env.get_object('box0')
handjnt = r.get_joint('RARM_JOINT5')
reltf = (-handjnt.where())*tgtobj.where()
tgtobj.unfix()
tgtobj.affix(handjnt, reltf)

# 手先軌道での動作生成 (収束IKではなく、初期姿勢に近い解を明示的に選択している)

# 軌道の作成
traj = CoordinateObjects('trajectory')
traj.append( ... ) # 適当なフレームを追加する
env.insert_object(traj, FRAME(), env.get_world()) # 世界座標相対で軌道を定義
traj.coords

# 削除したいときは
env.delete_object('trajectory')

# 7, 実機を動かす

# 実機とのインタフェース

rr = RealHIRO()
rr.connect() # 認識処理が始まっていなければ開始指令を送る
rr.get_joint_angles() # tf and joint state by ROS
js = r.get_joint_angles()
# socket(+pickle) to jython script
# scequencer, wait
duration = 4.0
rr.send_goal(js, 4.0) # blocking
rr.send_goal(js, 4.0, wait=False) # non-blocking
# blockingな呼出しも最大10[sec]でtimeoutする仕様

# 実際には、モデルで姿勢を確認してから以下を実行するのが便利
sync() # デフォルトは4[sec]で実機をモデルに同期させる
sync(duration=3.0) # 時間を変える

# チェッカーボードの認識
detect() # 認識結果がboxとして表示される
objfrm = detect() # 実機の場合はROSの識別器からtfを受信
                  # simulationの場合は直接、物体位置を読む

# Linkに沿った座標変換
# 認識結果は、カメラ=>対象物の座標変換であるので、
# 世界座標=>対象物の座標変換に直す
# detect()関数の中で行っている処理
r.Thd_leye # 頭リンク => 左目カメラへの変換
Tleye_obj = rr.detect()
Twld_hd = r.get_link('HEAD_JOINT1_Link').where()
Twld_obj = Twld_hd * r.Thd_leye * Tleye_obj

# ハンドカメラの利用
# 右手ハンドカメラで2個の箱を探し、
# マーカ1番(隅が欠けている方)の箱を2番の箱の上に置く

# 実機の場合
hand_cam_demo()

# シミュレータで実行するときは，先に手が届く場所に箱を置いておく
# 'box0' => marker 0, 'box1' => marker 1 に対応
putbox(name='box0', vaxis='y')
putbox(name='box1', vaxis='y')
hand_cam_demo()

# もう一度実行したいとき
env.delete_object('box0')
putbox(name='box0', vaxis='y')


# ハンドカメラはAR-toolkitによるmarker認識(複数物体対応)
# 返り値は (マーカ番号、カメラ=>マーカの座標変換)のリスト
# non-blockingで最新の認識結果を返す
# 過去 thre [sec]以内に認識に成功していないマーカは返さない
# デフォルトは thre = 0.5[sec]

# Linuxのドライバの問題で1台のPCで2つのハンドカメラデバイスを開けない
# 2台のPCに1つずつつないでいる
# 左手カメラを使うときには、まず左手カメラのキャプチャと認識プログラムを走らせる必要がある
lupus@ roslaunch Sense sense_lhand_ar.launch
rr.detect(camera='lhand') # 左手カメラによる認識

# 平行グリッパの間隔指定把持
r.grasp(width=65, hand='right') # 指の間隔がwidth[mm]になるように指を動かす
r.grasp(65) # これでも同じ

# アプローチ距離の指定
pl.reaching_plan(objfrm, approach_distance=80) # 少し遠くからアプローチ


# 練習：
#  手や首を動かして、目の前以外に置かれた箱を掴む
#  別の箱を積む
#  違うもの(例えば大きさが異なる箱、ペットボトル)を掴んでみる
#    対象物位置は既知とする
#    アプローチ、把持姿勢の計算
#    ハンドの把持角度 or 指先間隔の設定
#    実機での動作実験 (机に当たらないように注意)
#  箱を並べる(押す、滑らせる)
#  ハノイの塔をやってみる


# 動作計画(仕様が変わる可能性大)
traj = test_plan() # RRT-connectによる動作計画
show_traj(traj) # 軌道の表示
opttraj = pl.optimize_trajectory(traj) # 軌道の最適化
show_traj(opttraj)
opttraj = pl.optimize_trajectory(opttraj) # さらにスムージング

# 干渉チェックにはPQPを利用
# ロボットのモデルVRMLそのまま
# 環境はbox, AABBのみ (AABB=>triangular mesh=>PQP)

# 干渉チェック対象物の追加例
r.add_collision_object(env.get_object('table top'))

\end{verbatim}

% \addcontentsline{toc}{chapter}{参考文献}
% \markboth{参考文献}{参考文献}
% \bibliographystyle{junsrt}
% \bibliography{p2009}


\end{document}
